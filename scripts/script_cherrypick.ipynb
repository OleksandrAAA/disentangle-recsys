{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPsgHBFz0v6R/84uVa5soWW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OleksandrAAA/disentangle-recsys/blob/main/scripts/script_cherrypick.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import unicode_literals\n",
        "\n",
        "import subprocess\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "interpretable = 0  # the higher, the more interpretable\n",
        "\n",
        "# Macro disentanglement is good when interpretable >=2,\n",
        "# while good micro disentanglement requires a higher score.\n",
        "\n",
        "while interpretable < 2:\n",
        "    seed = np.random.randint(0, 1 << 30)\n",
        "    cmd = 'CUDA_VISIBLE_DEVICES=3 python3 main.py '\n",
        "    cmd += '--data data/alishop --epoch 20 --seed %d --mode %s'\n",
        "    try:\n",
        "        subprocess.check_output(cmd % (seed, 'trn'), shell=True)\n",
        "        val = subprocess.check_output(cmd % (seed, 'vis'), shell=True)\n",
        "        val = eval(val)\n",
        "        print('interpretable=%d, seed=%d' % (val, seed))\n",
        "    except subprocess.CalledProcessError:\n",
        "        val = 0\n",
        "    interpretable = max(interpretable, val)\n"
      ],
      "metadata": {
        "id": "5epNU-WgF5VF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0_bt0SNEcLv"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\n",
        "# Code developed based on https://github.com/dawenl/vae_cf/\n",
        "\n",
        "#  We also have a distributed version that uses sampled softmax.\n",
        "#  However, that version requires Alibaba's internal infrastructure.\n",
        "#  I'll see if I could manage to release the core part of that version\n",
        "#  while removing the Alibaba-specific parts.\n",
        "\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import unicode_literals\n",
        "\n",
        "import argparse\n",
        "import csv\n",
        "import os\n",
        "import shutil\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import bottleneck as bn\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn.decomposition\n",
        "import sklearn.manifold\n",
        "import sklearn.preprocessing\n",
        "import tensorflow as tf\n",
        "from scipy import sparse\n",
        "import tensorflow_probability as tfp\n",
        "#from tensorflow.contrib.distributions import RelaxedOneHotCategorical\n",
        "#from tensorflow.contrib.layers import apply_regularization, l2_regularizer\n",
        "#import tf_slim as slim\n",
        "ARG = argparse.ArgumentParser()\n",
        "ARG.add_argument('--data', type=str, required=True,\n",
        "                 help='./data/ml-latest-small, ./data/ml-1m, '\n",
        "                      './data/ml-20m, or ./data/alishop-7c')\n",
        "ARG.add_argument('--mode', type=str, default='trn',\n",
        "                 help='trn/tst/vis, for training/testing/visualizing.')\n",
        "ARG.add_argument('--logdir', type=str, default='./runs/')\n",
        "ARG.add_argument('--seed', type=int, default=98765,\n",
        "                 help='Random seed. Ignored if < 0.')\n",
        "ARG.add_argument('--epoch', type=int, default=200,\n",
        "                 help='Number of training epochs.')\n",
        "ARG.add_argument('--batch', type=int, default=100,\n",
        "                 help='Training batch size.')\n",
        "ARG.add_argument('--lr', type=float, default=1e-3,\n",
        "                 help='Initial learning rate.')\n",
        "ARG.add_argument('--rg', type=float, default=0.0,\n",
        "                 help='L2 regularization.')\n",
        "ARG.add_argument('--keep', type=float, default=0.5,\n",
        "                 help='Keep probability for dropout, in (0,1].')\n",
        "ARG.add_argument('--beta', type=float, default=0.2,\n",
        "                 help='Strength of disentanglement, in (0,oo).')\n",
        "ARG.add_argument('--tau', type=float, default=0.1,\n",
        "                 help='Temperature of sigmoid/softmax, in (0,oo).')\n",
        "ARG.add_argument('--std', type=float, default=0.075,\n",
        "                 help='Standard deviation of the Gaussian prior.')\n",
        "#ARG.add_argument('--kfac', type=int, default=7,\n",
        "#                 help='Number of facets (macro concepts).')\n",
        "ARG.add_argument('--kfac', type=int, default=100,\n",
        "                 help='Number of facets (macro concepts).')\n",
        "ARG.add_argument('--dfac', type=int, default=100,\n",
        "                 help='Dimension of each facet.')\n",
        "ARG.add_argument('--nogb', action='store_true', default=False,\n",
        "                 help='Disable Gumbel-Softmax sampling.')\n",
        "ARG = ARG.parse_args()\n",
        "\n",
        "if ARG.seed < 0:\n",
        "    ARG.seed = int(time.time())\n",
        "LOG_DIR = '%s-%dT-%dB-%glr-%grg-%gkp-%gb-%gt-%gs-%dk-%dd-%d' % (\n",
        "    ARG.data.replace('-', '/'), ARG.epoch, ARG.batch, ARG.lr, ARG.rg, ARG.keep,\n",
        "    ARG.beta, ARG.tau, ARG.std, ARG.kfac, ARG.dfac, ARG.seed)\n",
        "if ARG.nogb:\n",
        "    LOG_DIR += '-nogb'\n",
        "LOG_DIR = os.path.join(ARG.logdir, LOG_DIR)\n",
        "\n",
        "batch_size_vad = batch_size_test = ARG.batch\n",
        "\n",
        "\n",
        "def set_rng_seed(seed):\n",
        "    np.random.seed(seed)\n",
        "    tf.compat.v1.set_random_seed(seed)\n",
        "\n",
        "\n",
        "class MyVAE(object):\n",
        "    def __init__(self, num_items):\n",
        "        kfac, dfac = ARG.kfac, ARG.dfac\n",
        "        self.lam = ARG.rg\n",
        "        self.lr = ARG.lr\n",
        "        self.random_seed = ARG.seed\n",
        "\n",
        "        self.n_items = num_items\n",
        "\n",
        "        # The first fc layer of the encoder Q is the context embedding table.\n",
        "        self.q_dims = [num_items, dfac, dfac]\n",
        "        self.weights_q, self.biases_q = [], []\n",
        "        for i, (d_in, d_out) in enumerate(\n",
        "                zip(self.q_dims[:-1], self.q_dims[1:])):\n",
        "            if i == len(self.q_dims[:-1]) - 1:\n",
        "                d_out *= 2  # mu & var\n",
        "            weight_key = \"weight_q_{}to{}\".format(i, i + 1)\n",
        "            self.weights_q.append(tf.compat.v1.get_variable(\n",
        "                name=weight_key, shape=[d_in, d_out],\n",
        "                initializer=tf.compat.v1.keras.initializers.VarianceScaling(\n",
        "                    scale=1.0, mode=\"fan_avg\", distribution=\"uniform\", seed=self.random_seed)))\n",
        "            bias_key = \"bias_q_{}\".format(i + 1)\n",
        "            self.biases_q.append(tf.compat.v1.get_variable(\n",
        "                name=bias_key, shape=[d_out],\n",
        "                initializer=tf.compat.v1.truncated_normal_initializer(\n",
        "                    stddev=0.001, seed=self.random_seed)))\n",
        "\n",
        "        self.items = tf.compat.v1.get_variable(\n",
        "            name=\"items\", shape=[num_items, dfac],\n",
        "            initializer=tf.compat.v1.keras.initializers.VarianceScaling(\n",
        "                scale=1.0, mode=\"fan_avg\", distribution=\"uniform\", seed=self.random_seed))\n",
        "\n",
        "        self.cores = tf.compat.v1.get_variable(\n",
        "            name=\"cores\", shape=[kfac, dfac],\n",
        "            initializer=tf.compat.v1.keras.initializers.VarianceScaling(\n",
        "                scale=1.0, mode=\"fan_avg\", distribution=\"uniform\", seed=self.random_seed))\n",
        "\n",
        "        self.input_ph = tf.compat.v1.placeholder(\n",
        "            dtype=tf.float32, shape=[None, num_items])\n",
        "        self.keep_prob_ph = tf.compat.v1.placeholder_with_default(1., shape=None)\n",
        "        self.is_training_ph = tf.compat.v1.placeholder_with_default(0., shape=None)\n",
        "        self.anneal_ph = tf.compat.v1.placeholder_with_default(1., shape=None)\n",
        "\n",
        "    def build_graph(self, save_emb=False):\n",
        "        if save_emb:\n",
        "            saver, facets_list = self.forward_pass(save_emb=True)\n",
        "            return saver, facets_list, self.items, self.cores\n",
        "\n",
        "        saver, logits, recon_loss, kl = self.forward_pass(save_emb=False)\n",
        "\n",
        "        #reg_var = apply_regularization(l2_regularizer(self.lam), self.weights_q + [self.items, self.cores])\n",
        "        # tensorflow l2 regularization multiply 0.5 to the l2 norm\n",
        "        # multiply 2 so that it is back in the same scale\n",
        "        neg_elbo = recon_loss + self.anneal_ph * kl # + 2. * reg_var\n",
        "\n",
        "        train_op = tf.compat.v1.train.AdamOptimizer(self.lr).minimize(neg_elbo)\n",
        "\n",
        "        # add summary statistics\n",
        "        tf.compat.v1.summary.scalar('trn/neg_ll', recon_loss)\n",
        "        tf.compat.v1.summary.scalar('trn/kl_div', kl)\n",
        "        tf.compat.v1.summary.scalar('trn/neg_elbo', neg_elbo)\n",
        "        merged = tf.compat.v1.summary.merge_all()\n",
        "\n",
        "        return saver, logits, train_op, merged\n",
        "\n",
        "    def q_graph_k(self, x):\n",
        "        mu_q, std_q, kl = None, None, None\n",
        "        h = tf.nn.l2_normalize(x, 1)\n",
        "        h = tf.nn.dropout(h, 1 - (self.keep_prob_ph))\n",
        "        for i, (w, b) in enumerate(zip(self.weights_q, self.biases_q)):\n",
        "            h = tf.matmul(h, w, a_is_sparse=(i == 0)) + b\n",
        "            if i != len(self.weights_q) - 1:\n",
        "                h = tf.nn.tanh(h)\n",
        "            else:\n",
        "                mu_q = h[:, :self.q_dims[-1]]\n",
        "                mu_q = tf.nn.l2_normalize(mu_q, axis=1)\n",
        "                lnvarq_sub_lnvar0 = -h[:, self.q_dims[-1]:]\n",
        "                std0 = ARG.std\n",
        "                std_q = tf.exp(0.5 * lnvarq_sub_lnvar0) * std0\n",
        "                # Trick: KL is constant w.r.t. to mu_q after we normalize mu_q.\n",
        "                kl = tf.reduce_mean(input_tensor=tf.reduce_sum(\n",
        "                    input_tensor=0.5 * (-lnvarq_sub_lnvar0 + tf.exp(lnvarq_sub_lnvar0) - 1.),\n",
        "                    axis=1))\n",
        "        return mu_q, std_q, kl\n",
        "\n",
        "    def forward_pass(self, save_emb):\n",
        "        # clustering\n",
        "        print(\"clustering...1234\")\n",
        "        cores = tf.nn.l2_normalize(self.cores, axis=1)\n",
        "        items = tf.nn.l2_normalize(self.items, axis=1)\n",
        "        cates_logits = tf.matmul(items, cores, transpose_b=True) / ARG.tau\n",
        "        if ARG.nogb:\n",
        "            cates = tf.nn.softmax(cates_logits, axis=1)\n",
        "        else:\n",
        "            cates_dist = tfp.distributions.RelaxedOneHotCategorical(1, cates_logits)\n",
        "            cates_sample = cates_dist.sample()\n",
        "            cates_mode = tf.nn.softmax(cates_logits, axis=1)\n",
        "            cates = (self.is_training_ph * cates_sample +\n",
        "                     (1 - self.is_training_ph) * cates_mode)\n",
        "\n",
        "        z_list = []\n",
        "        probs, kl = None, None\n",
        "        for k in range(ARG.kfac):\n",
        "            cates_k = tf.reshape(cates[:, k], (1, -1))\n",
        "\n",
        "            # q-network\n",
        "            x_k = self.input_ph * cates_k\n",
        "            mu_k, std_k, kl_k = self.q_graph_k(x_k)\n",
        "            epsilon = tf.random.normal(tf.shape(input=std_k))\n",
        "            z_k = mu_k + self.is_training_ph * epsilon * std_k\n",
        "            kl = (kl_k if (kl is None) else (kl + kl_k))\n",
        "            if save_emb:\n",
        "                z_list.append(z_k)\n",
        "\n",
        "            # p-network\n",
        "            z_k = tf.nn.l2_normalize(z_k, axis=1)\n",
        "            logits_k = tf.matmul(z_k, items, transpose_b=True) / ARG.tau\n",
        "            probs_k = tf.exp(logits_k)\n",
        "            probs_k = probs_k * cates_k\n",
        "            probs = (probs_k if (probs is None) else (probs + probs_k))\n",
        "\n",
        "        logits = tf.math.log(probs)\n",
        "        logits = tf.nn.log_softmax(logits)\n",
        "        recon_loss = tf.reduce_mean(input_tensor=tf.reduce_sum(\n",
        "            input_tensor=-logits * self.input_ph, axis=-1))\n",
        "\n",
        "        if save_emb:\n",
        "            return tf.compat.v1.train.Saver(), z_list\n",
        "        return tf.compat.v1.train.Saver(), logits, recon_loss, kl\n",
        "\n",
        "\n",
        "def load_data(data_dir):\n",
        "    pro_dir = os.path.join(data_dir, 'pro_sg')\n",
        "\n",
        "    unique_sid = list()\n",
        "    with open(os.path.join(pro_dir, 'unique_sid.txt'), 'r') as f:\n",
        "        for line in f:\n",
        "            unique_sid.append(line.strip())\n",
        "    n_items = len(unique_sid)\n",
        "\n",
        "    train_data = load_train_data(os.path.join(pro_dir, 'train.csv'), n_items)\n",
        "\n",
        "    vad_data_tr, vad_data_te = load_tr_te_data(\n",
        "        os.path.join(pro_dir, 'validation_tr.csv'),\n",
        "        os.path.join(pro_dir, 'validation_te.csv'),\n",
        "        n_items)\n",
        "\n",
        "    tst_data_tr, tst_data_te = load_tr_te_data(\n",
        "        os.path.join(pro_dir, 'test_tr.csv'),\n",
        "        os.path.join(pro_dir, 'test_te.csv'),\n",
        "        n_items)\n",
        "    print(n_items)\n",
        "    print(train_data.shape[1])\n",
        "    assert n_items == train_data.shape[1]\n",
        "    assert n_items == vad_data_tr.shape[1]\n",
        "    assert n_items == vad_data_te.shape[1]\n",
        "    assert n_items == tst_data_tr.shape[1]\n",
        "    assert n_items == tst_data_te.shape[1]\n",
        "\n",
        "    return (n_items, train_data, vad_data_tr, vad_data_te,\n",
        "            tst_data_tr, tst_data_te)\n",
        "\n",
        "\n",
        "def load_train_data(csv_file, n_items):\n",
        "    tp = pd.read_csv(csv_file)\n",
        "    n_users = tp['uid'].max() + 1\n",
        "\n",
        "    rows, cols = tp['uid'], tp['sid']\n",
        "    #print(\"rows\",rows)\n",
        "    print(\"cols\",cols.max())\n",
        "    print(\"n_users\", int(n_users))\n",
        "    print(\"n_items\",int(n_items))\n",
        "    data = sparse.csr_matrix((np.ones_like(rows),\n",
        "                              (rows, cols)), dtype='float32',\n",
        "                             shape=(int(n_users), int(n_items)))\n",
        "    return data\n",
        "\n",
        "\n",
        "def load_tr_te_data(csv_file_tr, csv_file_te, n_items):\n",
        "    tp_tr = pd.read_csv(csv_file_tr)\n",
        "    tp_te = pd.read_csv(csv_file_te)\n",
        "\n",
        "    start_idx = min(tp_tr['uid'].min(), tp_te['uid'].min())\n",
        "    end_idx = max(tp_tr['uid'].max(), tp_te['uid'].max())\n",
        "\n",
        "    rows_tr, cols_tr = tp_tr['uid'] - start_idx, tp_tr['sid']\n",
        "    rows_te, cols_te = tp_te['uid'] - start_idx, tp_te['sid']\n",
        "    \n",
        "    print(\"======\")\n",
        "    print( \"min uid te\" , tp_te['uid'].min())\n",
        "    print(\"min uid tr\",tp_tr['uid'].min())\n",
        "    print( \"max uid te\" , tp_te['uid'].max())\n",
        "    print( \"max uid tr\" , tp_tr['uid'].max())\n",
        "\n",
        "    print(\"rows_te size\" , len(rows_te))\n",
        "    print(\"cols_te size\" ,len(cols_te))\n",
        "    print(\"start_idx\", start_idx)\n",
        "    print(\"end_idx\", end_idx)\n",
        "    print(\"n_items\",n_items)\n",
        "    print(\"size\" ,end_idx - start_idx + 1)\n",
        "    print(\"******\")\n",
        "    data_tr = sparse.csr_matrix((np.ones_like(rows_tr),\n",
        "                                 (rows_tr, cols_tr)), dtype='float64',\n",
        "                                shape=(end_idx - start_idx + 1, n_items))\n",
        "    data_te = sparse.csr_matrix((np.ones_like(rows_te),\n",
        "                                 (rows_te, cols_te)), dtype='float64',\n",
        "                                shape=(end_idx - start_idx + 1, n_items))\n",
        "    return data_tr, data_te\n",
        "\n",
        "\n",
        "def load_item_cate(data_dir, num_items):\n",
        "    #assert 'alishop' in data_dir\n",
        "    data_dir = os.path.join(data_dir, 'pro_sg')\n",
        "\n",
        "    hash_to_sid = {}\n",
        "    with open(os.path.join(data_dir, 'unique_sid.txt')) as fin:\n",
        "        for i, line in enumerate(fin):\n",
        "            hash_to_sid[int(line)] = i\n",
        "    assert num_items == len(hash_to_sid)\n",
        "\n",
        "    hash_to_cid = {}\n",
        "    with open(os.path.join(data_dir, 'item_cate.csv')) as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        for item, cate in reader:\n",
        "            item, cate = int(item), int(cate)\n",
        "            if item not in hash_to_sid:\n",
        "                continue\n",
        "            assert item in hash_to_sid\n",
        "            if cate not in hash_to_cid:\n",
        "                hash_to_cid[cate] = len(hash_to_cid)\n",
        "    num_cates = len(hash_to_cid)\n",
        "\n",
        "    item_cate = np.zeros((num_items, num_cates), dtype=np.bool)\n",
        "    with open(os.path.join(data_dir, 'item_cate.csv')) as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        for item, cate in reader:\n",
        "            item, cate = int(item), int(cate)\n",
        "            if item not in hash_to_sid:\n",
        "                continue\n",
        "            item = hash_to_sid[item]\n",
        "            cate = hash_to_cid[cate]\n",
        "            item_cate[item, cate] = True\n",
        "    item_cate = item_cate.astype(np.int64)\n",
        "\n",
        "    js = np.argsort(item_cate.sum(axis=0))[-7:]\n",
        "    item_cate = item_cate[:, js]\n",
        "    assert np.min(np.sum(item_cate, axis=1)) == 1\n",
        "    assert np.max(np.sum(item_cate, axis=1)) == 1\n",
        "    return item_cate\n",
        "\n",
        "\n",
        "def ndcg_binary_at_k_batch(x_pred, heldout_batch, k=100):\n",
        "    \"\"\"\n",
        "    normalized discounted cumulative gain@k for binary relevance\n",
        "    ASSUMPTIONS: all the 0's in heldout_data indicate 0 relevance\n",
        "    \"\"\"\n",
        "    batch_users = x_pred.shape[0]\n",
        "    idx_topk_part = bn.argpartition(-x_pred, k, axis=1)\n",
        "    topk_part = x_pred[np.arange(batch_users)[:, np.newaxis],\n",
        "                       idx_topk_part[:, :k]]\n",
        "    idx_part = np.argsort(-topk_part, axis=1)\n",
        "    # X_pred[np.arange(batch_users)[:, np.newaxis], idx_topk] is the sorted\n",
        "    # topk predicted score\n",
        "    idx_topk = idx_topk_part[np.arange(batch_users)[:, np.newaxis], idx_part]\n",
        "    # build the discount template\n",
        "    tp = 1. / np.log2(np.arange(2, k + 2))\n",
        "\n",
        "    dcg = (heldout_batch[np.arange(batch_users)[:, np.newaxis],\n",
        "                         idx_topk].toarray() * tp).sum(axis=1)\n",
        "    idcg = np.array([(tp[:min(n, k)]).sum()\n",
        "                     for n in heldout_batch.getnnz(axis=1)])\n",
        "    ndcg = dcg / idcg\n",
        "    ndcg[np.isnan(ndcg)] = 0\n",
        "    return ndcg\n",
        "\n",
        "\n",
        "def recall_at_k_batch(x_pred, heldout_batch, k=100):\n",
        "    batch_users = x_pred.shape[0]\n",
        "\n",
        "    idx = bn.argpartition(-x_pred, k, axis=1)\n",
        "    x_pred_binary = np.zeros_like(x_pred, dtype=bool)\n",
        "    x_pred_binary[np.arange(batch_users)[:, np.newaxis], idx[:, :k]] = True\n",
        "\n",
        "    x_true_binary = (heldout_batch > 0).toarray()\n",
        "    tmp = (np.logical_and(x_true_binary, x_pred_binary).sum(axis=1)).astype(\n",
        "        np.float32)\n",
        "    recall = tmp / np.minimum(k, x_true_binary.sum(axis=1))\n",
        "    recall[np.isnan(recall)] = 0\n",
        "    return recall\n",
        "\n",
        "\n",
        "def main_trn(train_data, vad_data_tr, vad_data_te):\n",
        "    set_rng_seed(ARG.seed)\n",
        "\n",
        "    n = train_data.shape[0]\n",
        "    print(n)\n",
        "    n_items = train_data.shape[1]\n",
        "    idxlist = list(range(n))\n",
        "\n",
        "    n_vad = vad_data_tr.shape[0]\n",
        "    idxlist_vad = list(range(n_vad))\n",
        "\n",
        "    num_batches = int(np.ceil(float(n) / ARG.batch))\n",
        "    total_anneal_steps = 5 * num_batches\n",
        "\n",
        "    tf.compat.v1.reset_default_graph()\n",
        "    vae = MyVAE(n_items)\n",
        "    saver, logits_var, train_op_var, merged_var = vae.build_graph()\n",
        "\n",
        "    ndcg_var = tf.Variable(0.0)\n",
        "    ndcg_best_var = tf.compat.v1.placeholder(dtype=tf.float64, shape=None)\n",
        "    ndcg_summary = tf.compat.v1.summary.scalar('vad/ndcg', ndcg_var)\n",
        "    ndcg_best_summary = tf.compat.v1.summary.scalar('vad/ndcg_best', ndcg_best_var)\n",
        "    merged_valid = tf.compat.v1.summary.merge([ndcg_summary, ndcg_best_summary])\n",
        "\n",
        "    if os.path.exists(LOG_DIR):\n",
        "        shutil.rmtree(LOG_DIR)\n",
        "    summary_writer = tf.compat.v1.summary.FileWriter(LOG_DIR,\n",
        "                                           graph=tf.compat.v1.get_default_graph())\n",
        "    if not os.path.isdir(LOG_DIR):\n",
        "        os.makedirs(LOG_DIR)\n",
        "    from tqdm import tqdm\n",
        "    with tf.compat.v1.Session() as sess:\n",
        "        init = tf.compat.v1.global_variables_initializer()\n",
        "        sess.run(init)\n",
        "        best_ndcg = -np.inf\n",
        "        update_count = 0.0\n",
        "        for epoch in range(ARG.epoch):\n",
        "            print(\"Epoch ==========> \",epoch)\n",
        "            np.random.shuffle(idxlist)\n",
        "            for bnum, st_idx in enumerate(range(0, n, ARG.batch)):\n",
        "                end_idx = min(st_idx + ARG.batch, n)\n",
        "                x = train_data[idxlist[st_idx:end_idx]]\n",
        "                if sparse.isspmatrix(x):\n",
        "                    x = x.toarray()\n",
        "                x = x.astype('float32')\n",
        "                if total_anneal_steps > 0:\n",
        "                    anneal = min(ARG.beta,\n",
        "                                 1. * update_count / total_anneal_steps)\n",
        "                else:\n",
        "                    anneal = ARG.beta\n",
        "                feed_dict = {vae.input_ph: x,\n",
        "                             vae.keep_prob_ph: ARG.keep,\n",
        "                             vae.anneal_ph: anneal,\n",
        "                             vae.is_training_ph: 1}\n",
        "                sess.run(train_op_var, feed_dict=feed_dict)\n",
        "                if bnum % 100 == 0:\n",
        "                    summary_train = sess.run(merged_var, feed_dict=feed_dict)\n",
        "                    summary_writer.add_summary(\n",
        "                        summary_train,\n",
        "                        global_step=epoch * num_batches + bnum)\n",
        "                update_count += 1\n",
        "\n",
        "            ndcg_dist = []\n",
        "            for bnum, st_idx in enumerate(range(0, n_vad, batch_size_vad)):\n",
        "                end_idx = min(st_idx + batch_size_vad, n_vad)\n",
        "                x = vad_data_tr[idxlist_vad[st_idx:end_idx]]\n",
        "                if sparse.isspmatrix(x):\n",
        "                    x = x.toarray()\n",
        "                x = x.astype('float32')\n",
        "                pred_val = sess.run(logits_var, feed_dict={vae.input_ph: x})\n",
        "                # exclude examples from training and validation (if any)\n",
        "                pred_val[x.nonzero()] = -np.inf\n",
        "                ndcg_dist.append(\n",
        "                    ndcg_binary_at_k_batch(\n",
        "                        pred_val, vad_data_te[idxlist_vad[st_idx:end_idx]]))\n",
        "            ndcg_dist = np.concatenate(ndcg_dist)\n",
        "            ndcg = ndcg_dist.mean()\n",
        "            print(\"ndcg===>\",ndcg)\n",
        "            if ndcg > best_ndcg:\n",
        "                saver.save(sess, '{}/chkpt'.format(LOG_DIR))\n",
        "                best_ndcg = ndcg\n",
        "            merged_valid_val = sess.run(\n",
        "                merged_valid,\n",
        "                feed_dict={ndcg_var: ndcg, ndcg_best_var: best_ndcg})\n",
        "            summary_writer.add_summary(merged_valid_val, epoch)\n",
        "\n",
        "    return best_ndcg\n",
        "\n",
        "\n",
        "def main_tst(tst_data_tr, tst_data_te, report_r20=False):\n",
        "    set_rng_seed(ARG.seed)\n",
        "\n",
        "    n_test = tst_data_tr.shape[0]\n",
        "    n_items = tst_data_tr.shape[1]\n",
        "    idxlist_test = list(range(n_test))\n",
        "\n",
        "    tf.compat.v1.reset_default_graph()\n",
        "    vae = MyVAE(n_items)\n",
        "    saver, logits_var, _, _ = vae.build_graph()\n",
        "\n",
        "    n100_list, r20_list, r50_list = [], [], []\n",
        "    with tf.compat.v1.Session() as sess:\n",
        "        saver.restore(sess, '{}/chkpt'.format(LOG_DIR))\n",
        "        for bnum, st_idx in enumerate(range(0, n_test, batch_size_test)):\n",
        "            end_idx = min(st_idx + batch_size_test, n_test)\n",
        "            x = tst_data_tr[idxlist_test[st_idx:end_idx]]\n",
        "            if sparse.isspmatrix(x):\n",
        "                x = x.toarray()\n",
        "            x = x.astype('float32')\n",
        "            pred_val = sess.run(logits_var, feed_dict={vae.input_ph: x})\n",
        "            pred_val[x.nonzero()] = -np.inf\n",
        "            n100_list.append(ndcg_binary_at_k_batch(\n",
        "                pred_val, tst_data_te[idxlist_test[st_idx:end_idx]], k=100))\n",
        "            r20_list.append(recall_at_k_batch(\n",
        "                pred_val, tst_data_te[idxlist_test[st_idx:end_idx]], k=20))\n",
        "            r50_list.append(recall_at_k_batch(\n",
        "                pred_val, tst_data_te[idxlist_test[st_idx:end_idx]], k=50))\n",
        "\n",
        "    n100_list = np.concatenate(n100_list)\n",
        "    r20_list = np.concatenate(r20_list)\n",
        "    r50_list = np.concatenate(r50_list)\n",
        "\n",
        "    print(\"Test NDCG@100=%.5f (%.5f)\" % (\n",
        "        n100_list.mean(), np.std(n100_list) / np.sqrt(len(n100_list))),\n",
        "          file=sys.stderr)\n",
        "    print(\"Test Recall@20=%.5f (%.5f)\" % (\n",
        "        r20_list.mean(), np.std(r20_list) / np.sqrt(len(r20_list))),\n",
        "          file=sys.stderr)\n",
        "    print(\"Test Recall@50=%.5f (%.5f)\" % (\n",
        "        r50_list.mean(), np.std(r50_list) / np.sqrt(len(r50_list))),\n",
        "          file=sys.stderr)\n",
        "    if report_r20:\n",
        "        return r20_list.mean()\n",
        "    return n100_list.mean()\n",
        "\n",
        "\n",
        "def main_vis(train_data):\n",
        "    set_rng_seed(ARG.seed)\n",
        "\n",
        "    n = train_data.shape[0]\n",
        "    n_items = train_data.shape[1]\n",
        "    batch_size_vis = (ARG.batch + ARG.kfac - 1) // ARG.kfac\n",
        "\n",
        "    tf.compat.v1.reset_default_graph()\n",
        "    vae = MyVAE(n_items)\n",
        "    saver, facets_varls, items_var, cores_var = vae.build_graph(save_emb=True)\n",
        "    with tf.compat.v1.Session() as sess:\n",
        "        print(LOG_DIR)\n",
        "        print(sess)\n",
        "        saver.restore(sess, '{}/chkpt'.format(LOG_DIR))\n",
        "        items = sess.run(items_var)\n",
        "        cores = sess.run(cores_var)\n",
        "        users = []\n",
        "        for bnum, st_idx in enumerate(range(0, n, batch_size_vis)):\n",
        "            end_idx = min(st_idx + batch_size_vis, n)\n",
        "            x = train_data[st_idx:end_idx]\n",
        "            if sparse.isspmatrix(x):\n",
        "                x = x.toarray()\n",
        "            x = x.astype('float32')\n",
        "            facets_ls = sess.run(facets_varls, feed_dict={vae.input_ph: x})\n",
        "            users.append(np.concatenate(facets_ls, axis=1))\n",
        "        users = np.concatenate(users, axis=0)\n",
        "\n",
        "    interpretability = 0\n",
        "    interpretability += visualize_macro(users, items, cores, train_data)\n",
        "    return interpretability\n",
        "\n",
        "\n",
        "def np_normalize(x, axis=-1, eps=1e-12):\n",
        "    norm = np.linalg.norm(x, axis=axis, keepdims=True)\n",
        "    norm[norm < eps] = eps\n",
        "    return x / norm\n",
        "\n",
        "\n",
        "def matchness_of_cores_and_cates(cores, items, cate_items):\n",
        "    k = cate_items.shape[1]\n",
        "    cates = np.argmax(cate_items, axis=1)\n",
        "    cate_centers = []\n",
        "    for j in range(k):\n",
        "        cate_centers.append(items[cates == j].sum(axis=0, keepdims=True))\n",
        "    cate_centers = np.concatenate(cate_centers, axis=0)\n",
        "    cate_centers = np_normalize(cate_centers)\n",
        "    core_vs_cate = cores.dot(cate_centers.T)\n",
        "    print('core_vs_cate =\\n', core_vs_cate, file=sys.stderr)\n",
        "    best_cate_for_core = np.argmax(core_vs_cate, axis=1)\n",
        "    print('best_cate_for_core = ', best_cate_for_core, file=sys.stderr)\n",
        "    best_core_for_cate = np.argmax(core_vs_cate, axis=0)\n",
        "    print('best_core_for_cate = ', best_core_for_cate, file=sys.stderr)\n",
        "    interpretability = 0\n",
        "    if len(set(best_core_for_cate)) == k:\n",
        "        interpretability += 1\n",
        "    if len(set(best_cate_for_core)) == k:\n",
        "        interpretability += 1\n",
        "    if interpretability >= 2:\n",
        "        inconsistent = False\n",
        "        for j in range(k):\n",
        "            if best_core_for_cate[best_cate_for_core[j]] != j:\n",
        "                inconsistent = True\n",
        "                break\n",
        "        if not inconsistent:\n",
        "            interpretability += 1\n",
        "    return interpretability, best_cate_for_core\n",
        "\n",
        "\n",
        "def visualize_macro(users, items, cores, train_data):\n",
        "    palette = np.asarray(\n",
        "        [[238, 27., 39., 80],  # _0. Red\n",
        "         [59., 175, 81., 80],  # _1. Green\n",
        "         [255, 127, 38., 80],  # _2. Orange\n",
        "         [255, 129, 190, 80],  # _3. Pink\n",
        "         [153, 153, 153, 80],  # _4. Gray\n",
        "         [156, 78., 161, 80],  # _5. Purple\n",
        "         [35., 126, 181, 80]],  # 6. Blue\n",
        "        dtype=np.float32) / 255.0\n",
        "\n",
        "    n, m = users.shape[0], items.shape[0]\n",
        "    k, d = cores.shape\n",
        "    users = users.reshape(n, k, d)\n",
        "    assert items.shape[1] == d\n",
        "    del n\n",
        "\n",
        "    users = np_normalize(users)\n",
        "    items = np_normalize(items)\n",
        "    cores = np_normalize(cores)\n",
        "\n",
        "    cate_items = load_item_cate(ARG.data, m)\n",
        "    interpretable, core2cate = matchness_of_cores_and_cates(\n",
        "        cores, items, cate_items)\n",
        "    print('macro interpretable = %d [seed = %d]' % (\n",
        "        interpretable, ARG.seed),\n",
        "          file=sys.stderr)\n",
        "    if interpretable < 3:\n",
        "        print('Some prototypes do not align well with categories. '\n",
        "              'Maybe try another random seed? :)', file=sys.stderr)\n",
        "        return interpretable\n",
        "\n",
        "    cate_items = cate_items[:, core2cate]  # align categories with prototypes\n",
        "    gold_items = np.argmax(cate_items, axis=1)\n",
        "    pred_items = np.argmax(items.dot(cores.T), axis=1)\n",
        "\n",
        "    ufacs, gold_ufacs = [], []  # user facets\n",
        "    is_in_ufacs = set()\n",
        "    for u, i in zip(*train_data.nonzero()):\n",
        "        c = gold_items[i]\n",
        "        if (u, c) not in is_in_ufacs:\n",
        "            is_in_ufacs.add((u, c))\n",
        "            ufacs.append(users[u, c].reshape(1, d))\n",
        "            gold_ufacs.append(c)\n",
        "    del is_in_ufacs\n",
        "    ufacs = np.concatenate(ufacs, axis=0)\n",
        "    gold_ufacs = np.asarray(gold_ufacs, dtype=np.int64)\n",
        "    n = ufacs.shape[0]\n",
        "    assert ufacs.shape == (n, d)\n",
        "    assert gold_ufacs.shape == (n,)\n",
        "\n",
        "    vis_dir = os.path.join(LOG_DIR, 'vis')\n",
        "    if os.path.exists(vis_dir):\n",
        "        shutil.rmtree(vis_dir)\n",
        "\n",
        "    def plot(title, xy, color, marksz=2.):\n",
        "        fig, ax = plt.subplots()\n",
        "        ax.scatter(x=xy[:, 0], y=xy[:, 1], s=marksz, c=color)\n",
        "        plt.savefig('%s.png' % title.replace('/', '-'), format='png', dpi=160)\n",
        "\n",
        "    nodes = np.concatenate((items, ufacs), axis=0)\n",
        "    assert nodes.shape == (m + n, d)\n",
        "    gold_nodes = np.concatenate((gold_items, gold_ufacs), axis=0)\n",
        "    assert gold_nodes.shape == (m + n,)\n",
        "    pred_nodes = np.argmax(nodes.dot(cores.T), axis=1)\n",
        "\n",
        "    col_pred = palette[pred_nodes]\n",
        "    col_gold = palette[gold_nodes]\n",
        "\n",
        "    tsne2d_sav = os.path.join(LOG_DIR, 'tsne2d-nodes.npy')\n",
        "    if os.path.isfile(tsne2d_sav):\n",
        "        x_2d = np.load(tsne2d_sav)\n",
        "    else:\n",
        "        x_2d = nodes\n",
        "        if d > k:\n",
        "            x_2d = sklearn.decomposition.PCA(\n",
        "                n_components=k).fit_transform(x_2d)\n",
        "        print('Running tSNE...', file=sys.stderr)\n",
        "        x_2d = sklearn.manifold.TSNE(\n",
        "            n_components=2, perplexity=30).fit_transform(x_2d)\n",
        "        print('Finished tSNE...', file=sys.stderr)\n",
        "        np.save(tsne2d_sav, x_2d)\n",
        "    plot('tsne2d-nodes/pred', x_2d, col_pred)\n",
        "    plot('tsne2d-nodes/gold', x_2d, col_gold)\n",
        "    plot('tsne2d-items/pred', x_2d[:m], col_pred[:m])\n",
        "    plot('tsne2d-items/gold', x_2d[:m], col_gold[:m])\n",
        "    plot('tsne2d-users/pred', x_2d[m:], col_pred[m:])\n",
        "    plot('tsne2d-users/gold', x_2d[m:], col_gold[m:])\n",
        "\n",
        "    return interpretable\n",
        "\n",
        "\n",
        "def main():\n",
        "    #tf.config.list_physical_devices('GPU')\n",
        "    tf.compat.v1.disable_eager_execution()\n",
        "    (_, train_data, vad_data_tr, vad_data_te,\n",
        "     tst_data_tr, tst_data_te) = load_data(ARG.data)\n",
        "    val, tst = 0, 0\n",
        "    if ARG.mode in ('trn',):\n",
        "        val = main_trn(train_data, vad_data_tr, vad_data_te)\n",
        "    if ARG.mode in ('trn', 'tst'):\n",
        "        tst = main_tst(tst_data_tr, tst_data_te)\n",
        "        print('(%.5f, %.5f)' % (val, tst))\n",
        "    if ARG.mode in ('vis',):\n",
        "        how_interpretable = main_vis(train_data)\n",
        "        print(how_interpretable)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    }
  ]
}